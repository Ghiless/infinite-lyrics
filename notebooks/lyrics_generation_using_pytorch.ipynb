{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af175691c8999c43d0931cc180215bd1814f7bc1"
   },
   "source": [
    "# Lyrics generation using PyTorch\n",
    "\n",
    "This notebook is largly (laaaaaaaaargly!) inspired by [Cezanne Camacho's notebook](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb).\n",
    "\n",
    "In this notebook, we will try to create and train a model to generate lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "_uuid": "92261b3cdbfdb431e94e0bc879272422c4420417"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "669a52bf8041a4f69880947f9b86b9c0195327c3"
   },
   "source": [
    "First, let's explore our data and see its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "_uuid": "a0bd510b879499364603ec7c818465ce6a1e814e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                        ...                                                                       text\n",
       "0   ABBA                        ...                          Look at her face, it's a wonderful face  \\nAnd...\n",
       "1   ABBA                        ...                          Take it easy with me, please  \\nTouch me gentl...\n",
       "2   ABBA                        ...                          I'll never know why I had to go  \\nWhy I had t...\n",
       "3   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "4   ABBA                        ...                          Making somebody happy is a question of give an...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_csv(\"data/songdata.csv\")\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bc4c7f4095c550566b89fb830554b9f10205f63"
   },
   "source": [
    "What interests us the most here is the 'text' field. Our model will be training on it. After, we can choose to train a model on the texts of a specific artist.\n",
    "\n",
    "But before starting, let's have some fun and define a function that will print a random song each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "0c5d14f857a1b5a480b913bd989a6976ccc26059"
   },
   "outputs": [],
   "source": [
    "def print_random_song():\n",
    "    n = len(file)\n",
    "    song = file.loc[random.randrange(0, n)]\n",
    "    print(song['song'] + '\\nby ' + song['artist'] + '\\n\\n' + song['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "ebc3858fa03efb58768d8765205f61eb567cbe30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never Tear Us Apart\n",
      "by Natalie Imbruglia\n",
      "\n",
      "Don't ask me  \n",
      "What you know is true  \n",
      "Don't have to tell you  \n",
      "I love your precious heart  \n",
      "  \n",
      "I  \n",
      "I was standing  \n",
      "You were there  \n",
      "Two worlds collided  \n",
      "And they could never tear us apart  \n",
      "  \n",
      "We could live  \n",
      "For a thousand years  \n",
      "But if I hurt you  \n",
      "I'd make wine from your tears  \n",
      "  \n",
      "I told you  \n",
      "That we could fly  \n",
      "'Cause we all have wings  \n",
      "But some of us don't know why  \n",
      "  \n",
      "I  \n",
      "I was standing  \n",
      "You were there  \n",
      "Two worlds collided  \n",
      "And they could never ever tear us apart\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_random_song()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1973666465f5daaf972243d088d37cfd7778d45a"
   },
   "source": [
    "Now, let's get the full text concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "_uuid": "bfc521d12eac576cef01af5da3ecbc45f0fc6755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68056106"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = file['text'].str.cat(sep='\\n')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "baeb8957694cb7d4385028738aa5c0733f7a1234"
   },
   "source": [
    "We can see that we have over 60 million characters, which is a lot of characters. Training on that will take very long time. Let's just keep the first 2 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "_uuid": "a7f14fd5822a659d7702dd6e275732b0f4a252b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[:2000000]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "594f6cca2303815c1ca44d4e2c37fafdb76cabe9"
   },
   "source": [
    "This will exclude many talented artists but that's okay. We'll take them back when Kaggle gets better GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3d3591d2df3b131a79c66214b8e447d90b74bd5"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82160f11afed3157c3e080b74361bcfcdc81386d"
   },
   "source": [
    "Our model will not be able to train on our data as it is. We need to encode it. For that, we will translate each character into an integer. Let's start by retrieving all the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "_uuid": "b8ca57f20e034d2717d4861a2c62e60d5fb287c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters = sorted(tuple(set(text)))\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "n_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ba0bb2ac75d0a114adb800066a3834088f57291"
   },
   "source": [
    "Let's now define two dictionaries (one which maps integers to characters, another one wich maps charactes to intgers) and then, encode our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "_uuid": "46b1594befcb7b21b92377438f013d1981e0d8b3"
   },
   "outputs": [],
   "source": [
    "# given an index, get the character\n",
    "index_char = dict(enumerate(all_characters))\n",
    "\n",
    "# given a character, get the index\n",
    "char_index = {v: k for k, v in index_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "_uuid": "2ea9aee58144e1e0ee615b944d6def7b814c7465"
   },
   "outputs": [],
   "source": [
    "# encode the text\n",
    "encoded_text = np.array([char_index[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "_uuid": "915aaa2257c7cdc012709cce172eadba94116877"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 64, 64, 60,  1, 50, 69,  1, 57, 54, 67,  1, 55, 50, 52, 54,  7,\n",
       "        1, 58, 69,  4, 68,  1, 50,  1, 72, 64, 63, 53, 54, 67, 55, 70, 61,\n",
       "        1, 55, 50, 52, 54,  1,  1,  0, 22, 63, 53,  1, 58, 69,  1, 62, 54,\n",
       "       50, 63, 68,  1, 68, 64, 62, 54, 69, 57, 58, 63, 56,  1, 68, 65, 54,\n",
       "       52, 58, 50, 61,  1, 69, 64,  1, 62, 54,  1,  1,  0, 33, 64, 64, 60,\n",
       "        1, 50, 69,  1, 69, 57, 54,  1, 72, 50, 74,  1, 69, 57, 50])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "_uuid": "9ea96cbc6e5920554d38c31203424a47a2de8cdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way tha\""
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b81ed83322e0ca1d37f0a820ba3da72d3363efb1"
   },
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e179758900ff926958462d062f26ddfaca38437f"
   },
   "source": [
    "Defining a function to one-hot encode our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "_uuid": "b33fabc9eb42f129cd46822166d1074f4f9287f4"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels): \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c48376e1434916679fcbf984486e34f48c622ff8"
   },
   "source": [
    "## Making mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "_uuid": "485dff4262f19ef03c9e54ec8fc4524e36d8968d"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "351ab4956389a9f76d9dcc1f420dc44c1240d24a"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "_uuid": "ae7679e1de4f60a3bf4a81125b82844b98dbebd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "_uuid": "2344aa61241c1553cfd07f3120e001f277d454cd"
   },
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.index_char = dict(enumerate(self.chars))\n",
    "        self.char_index = {v: k for k, v in self.index_char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "_uuid": "bf2d89a8bfae1184b7ceb573f3ba985412e1f76e"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    all_losses = []\n",
    "    all_val_losses = []\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y).long()\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y.long()\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                all_losses.append(loss.item())\n",
    "                all_val_losses.append(np.mean(val_losses))\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "                \n",
    "    return all_losses, all_val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28991cbd5b3021be84144fb04a0f60f23800fbd2"
   },
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "_uuid": "ceeceee6e9321553ac7666a6ec35b198caf64c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (lstm): LSTM(76, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=76, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = MyRNN(all_characters, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "_uuid": "6f81efe8e5b8688ff5924e783565fc7434a0a4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 3.2109... Val Loss: 3.1975\n",
      "Epoch: 1/5... Step: 20... Loss: 3.1012... Val Loss: 3.0830\n",
      "Epoch: 1/5... Step: 30... Loss: 3.0698... Val Loss: 3.0750\n",
      "Epoch: 1/5... Step: 40... Loss: 3.0927... Val Loss: 3.0725\n",
      "Epoch: 1/5... Step: 50... Loss: 3.0862... Val Loss: 3.0690\n",
      "Epoch: 1/5... Step: 60... Loss: 3.0847... Val Loss: 3.0671\n",
      "Epoch: 1/5... Step: 70... Loss: 3.0845... Val Loss: 3.0642\n",
      "Epoch: 1/5... Step: 80... Loss: 3.0748... Val Loss: 3.0588\n",
      "Epoch: 1/5... Step: 90... Loss: 3.0451... Val Loss: 3.0408\n",
      "Epoch: 1/5... Step: 100... Loss: 3.0277... Val Loss: 3.0264\n",
      "Epoch: 1/5... Step: 110... Loss: 2.9940... Val Loss: 2.9763\n",
      "Epoch: 1/5... Step: 120... Loss: 2.8976... Val Loss: 2.8983\n",
      "Epoch: 1/5... Step: 130... Loss: 2.8237... Val Loss: 2.8046\n",
      "Epoch: 1/5... Step: 140... Loss: 2.7453... Val Loss: 2.7087\n",
      "Epoch: 2/5... Step: 150... Loss: 2.6702... Val Loss: 2.6177\n",
      "Epoch: 2/5... Step: 160... Loss: 2.5741... Val Loss: 2.5247\n",
      "Epoch: 2/5... Step: 170... Loss: 2.4502... Val Loss: 2.4349\n",
      "Epoch: 2/5... Step: 180... Loss: 2.4208... Val Loss: 2.3694\n",
      "Epoch: 2/5... Step: 190... Loss: 2.3643... Val Loss: 2.3099\n",
      "Epoch: 2/5... Step: 200... Loss: 2.3037... Val Loss: 2.2681\n",
      "Epoch: 2/5... Step: 210... Loss: 2.2996... Val Loss: 2.2318\n",
      "Epoch: 2/5... Step: 220... Loss: 2.2597... Val Loss: 2.2111\n",
      "Epoch: 2/5... Step: 230... Loss: 2.1994... Val Loss: 2.1786\n",
      "Epoch: 2/5... Step: 240... Loss: 2.2089... Val Loss: 2.1492\n",
      "Epoch: 2/5... Step: 250... Loss: 2.1840... Val Loss: 2.1233\n",
      "Epoch: 2/5... Step: 260... Loss: 2.1067... Val Loss: 2.1013\n",
      "Epoch: 2/5... Step: 270... Loss: 2.1241... Val Loss: 2.0798\n",
      "Epoch: 2/5... Step: 280... Loss: 2.1058... Val Loss: 2.0553\n",
      "Epoch: 3/5... Step: 290... Loss: 2.0922... Val Loss: 2.0384\n",
      "Epoch: 3/5... Step: 300... Loss: 2.0458... Val Loss: 2.0132\n",
      "Epoch: 3/5... Step: 310... Loss: 2.0088... Val Loss: 1.9929\n",
      "Epoch: 3/5... Step: 320... Loss: 2.0081... Val Loss: 1.9706\n",
      "Epoch: 3/5... Step: 330... Loss: 2.0245... Val Loss: 1.9505\n",
      "Epoch: 3/5... Step: 340... Loss: 1.9874... Val Loss: 1.9305\n",
      "Epoch: 3/5... Step: 350... Loss: 1.9766... Val Loss: 1.9258\n",
      "Epoch: 3/5... Step: 360... Loss: 1.9721... Val Loss: 1.9082\n",
      "Epoch: 3/5... Step: 370... Loss: 1.9198... Val Loss: 1.8887\n",
      "Epoch: 3/5... Step: 380... Loss: 1.9340... Val Loss: 1.8696\n",
      "Epoch: 3/5... Step: 390... Loss: 1.9015... Val Loss: 1.8561\n",
      "Epoch: 3/5... Step: 400... Loss: 1.8546... Val Loss: 1.8403\n",
      "Epoch: 3/5... Step: 410... Loss: 1.8697... Val Loss: 1.8277\n",
      "Epoch: 3/5... Step: 420... Loss: 1.8774... Val Loss: 1.8137\n",
      "Epoch: 4/5... Step: 430... Loss: 1.8788... Val Loss: 1.8018\n",
      "Epoch: 4/5... Step: 440... Loss: 1.8185... Val Loss: 1.7924\n",
      "Epoch: 4/5... Step: 450... Loss: 1.7973... Val Loss: 1.7798\n",
      "Epoch: 4/5... Step: 460... Loss: 1.8022... Val Loss: 1.7686\n",
      "Epoch: 4/5... Step: 470... Loss: 1.8327... Val Loss: 1.7589\n",
      "Epoch: 4/5... Step: 480... Loss: 1.8118... Val Loss: 1.7492\n",
      "Epoch: 4/5... Step: 490... Loss: 1.8016... Val Loss: 1.7389\n",
      "Epoch: 4/5... Step: 500... Loss: 1.7891... Val Loss: 1.7306\n",
      "Epoch: 4/5... Step: 510... Loss: 1.7639... Val Loss: 1.7212\n",
      "Epoch: 4/5... Step: 520... Loss: 1.7805... Val Loss: 1.7170\n",
      "Epoch: 4/5... Step: 530... Loss: 1.7398... Val Loss: 1.7050\n",
      "Epoch: 4/5... Step: 540... Loss: 1.6988... Val Loss: 1.6994\n",
      "Epoch: 4/5... Step: 550... Loss: 1.7180... Val Loss: 1.6911\n",
      "Epoch: 4/5... Step: 560... Loss: 1.7510... Val Loss: 1.6803\n",
      "Epoch: 5/5... Step: 570... Loss: 1.7497... Val Loss: 1.6751\n",
      "Epoch: 5/5... Step: 580... Loss: 1.7018... Val Loss: 1.6736\n",
      "Epoch: 5/5... Step: 590... Loss: 1.6731... Val Loss: 1.6645\n",
      "Epoch: 5/5... Step: 600... Loss: 1.6907... Val Loss: 1.6574\n",
      "Epoch: 5/5... Step: 610... Loss: 1.7097... Val Loss: 1.6539\n",
      "Epoch: 5/5... Step: 620... Loss: 1.6934... Val Loss: 1.6463\n",
      "Epoch: 5/5... Step: 630... Loss: 1.6922... Val Loss: 1.6362\n",
      "Epoch: 5/5... Step: 640... Loss: 1.6883... Val Loss: 1.6299\n",
      "Epoch: 5/5... Step: 650... Loss: 1.6590... Val Loss: 1.6237\n",
      "Epoch: 5/5... Step: 660... Loss: 1.6682... Val Loss: 1.6200\n",
      "Epoch: 5/5... Step: 670... Loss: 1.6465... Val Loss: 1.6168\n",
      "Epoch: 5/5... Step: 680... Loss: 1.6141... Val Loss: 1.6161\n",
      "Epoch: 5/5... Step: 690... Loss: 1.6215... Val Loss: 1.6051\n",
      "Epoch: 5/5... Step: 700... Loss: 1.6574... Val Loss: 1.5961\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5\n",
    "\n",
    "# train the model\n",
    "losses, val_losses = train(net, encoded_text, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0982b83cd985f7a340996422e5f1766966a489e"
   },
   "source": [
    "## Plot training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "_uuid": "3aaf48da863764d571fba822e7618b545fe375d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb69b6a3d68>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "407696a18efe90c42eab4b1f05c57142546d978e"
   },
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "_uuid": "cd38e5d472ae2aef4a1359f62fa748e10ce2a641"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char_index[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.index_char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "_uuid": "fb70ddc660f88ac6935cf33165f876d7f6df9378"
   },
   "outputs": [],
   "source": [
    "def sample_characters(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "_uuid": "b464d9bad180c554a0421a4a5873b7126e1c9728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you wanting this  \n",
      "And they gave me so mack to take you  \n",
      "Take me helen  \n",
      "  \n",
      "The say to me a stoppin\n"
     ]
    }
   ],
   "source": [
    "print(sample_characters(net, 100, prime='I love', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "_uuid": "a91e5769a0a187a419caf3614c7a3580a82a4ec6"
   },
   "outputs": [],
   "source": [
    "def sample_words(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    while 1:\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "        if len(''.join(chars).split()) == size + 1:\n",
    "            break;\n",
    "\n",
    "    return ''.join(chars[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "_uuid": "b1ef4aa784340d8a1b50ddb7d10648eadf647668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellonge  \n",
      "  \n",
      "That you're live in the strom on your hare  \n",
      "It's arlay, then I wan talk anow won some  \n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(sample_words(net, 20, prime='Hello', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "_uuid": "f847323c8483983c4208804a216d72ce08db6f5a"
   },
   "outputs": [],
   "source": [
    "def sample_lines(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    while (size > 0):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)        \n",
    "        if char == '\\n':\n",
    "            size -= 1\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "_uuid": "d529d62253a54d6bf2edc94ae0d6c98095c9fddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you wing they won't you to the trun  \n",
      "I was that you say I'm good  \n",
      "To my seeming say and me thit to be  \n",
      "If'r thinkses who wancer to me to be  \n",
      "It and the crows a minster alithe  \n",
      "I gotto bod a stared and was allot man  \n",
      "You've got the canders to me  \n",
      "You're groust woreds  \n",
      "In your lest all stream  \n",
      "I get the tryer  \n",
      "  \n",
      "It's all the way  \n",
      "If I ceard around you are  \n",
      "The wonth the truss of simess this in  \n",
      "A string all me aroughs  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample_lines(net, 15, prime='I love', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01860e280647889f1bed28bb6aa104aad9f481a"
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "_uuid": "a70c5f550b4818ec4142104d7b8b3e9d6bce2c4a"
   },
   "outputs": [],
   "source": [
    "model_name = 'models/model1.pth'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9296fd5477d58f0361072623a7bac2a51f44115"
   },
   "source": [
    "## Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "_uuid": "4b03ef71c266196457fbd56f0e111e5200e7a61d"
   },
   "outputs": [],
   "source": [
    "with open('models/model1.pth', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = MyRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
